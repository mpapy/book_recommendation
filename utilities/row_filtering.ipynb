{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "504f8ce1-3bff-4521-b4ec-40aa893d17cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "env = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b08a148f-054a-490a-a2be-28aa36cc9297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ——————————————————————————————\n",
    "#  ENV SETUP\n",
    "# ——————————————————————————————\n",
    "#env = dbutils.widgets.get(\"pipeline.env\")\n",
    "print(f\"Setting up security policies for environment: {env}\")\n",
    "\n",
    "catalog = \"book_rec_catalog\"\n",
    "schema = f\"{env}_silver\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {schema}\")\n",
    "\n",
    "print(f\"Using catalog: {catalog}\")\n",
    "print(f\"Using schema: {schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49e36cf-9579-4f91-bfd7-b236a0edb28d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ——————————————————————————————\n",
    "#  CREATE SECURITY FUNCTIONS\n",
    "# ——————————————————————————————\n",
    "\n",
    "print(\"Creating security functions...\")\n",
    "\n",
    "# Jedna masking funkce pro User-ID\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION {catalog}.{schema}.mask_user_id(user_id STRING)\n",
    "RETURNS STRING\n",
    "RETURN CASE \n",
    "    WHEN is_account_group_member('data_scientists') THEN user_id\n",
    "    WHEN is_account_group_member('analysts') THEN CONCAT('USER_', SUBSTRING(user_id, 1, 4), '***')\n",
    "    ELSE '*** REDACTED ***'\n",
    "END;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7181b54-2d83-469c-b7c3-89dd86105ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ——————————————————————————————\n",
    "#  CREATE ROW FILTERING\n",
    "# ——————————————————————————————\n",
    "# Jedna row filtering funkce pro geografické omezení\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION {catalog}.{schema}.rf_approved_regions(state STRING)\n",
    "RETURNS BOOLEAN\n",
    "RETURN CASE \n",
    "    WHEN is_account_group_member('viewers') THEN state = 'USA'\n",
    "    WHEN is_account_group_member('analysts') THEN state IN ('USA', 'Canada', 'UK')\n",
    "    ELSE TRUE\n",
    "END;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b01a54-ceb3-4c48-8224-ee491d217b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ——————————————————————————————\n",
    "#  APPLY SECURITY POLICIES\n",
    "# ——————————————————————————————\n",
    "\n",
    "print(\"Applying security policies...\")\n",
    "\n",
    "# Column masking na User-ID\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "    ALTER TABLE {catalog}.{schema}.users_silver_batch\n",
    "    ALTER COLUMN `User-ID`\n",
    "    SET MASK {catalog}.{schema}.mask_user_id\n",
    "    \"\"\")\n",
    "    print(\"Applied column masking to users_silver_batch.User-ID\")\n",
    "except Exception as e:\n",
    "    print(f\"Error applying column masking: {str(e)}\")\n",
    "\n",
    "# Row filtering na state\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "    ALTER TABLE {catalog}.{schema}.users_silver_batch\n",
    "    SET ROW FILTER {catalog}.{schema}.rf_approved_regions\n",
    "    ON (state)\n",
    "    \"\"\")\n",
    "    print(\"Applied row filtering to users_silver_batch on state\")\n",
    "except Exception as e:\n",
    "    print(f\"Error applying row filtering: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "row_filtering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
